{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ujyNeELoVxZn","outputId":"c6b58283-9381-46f7-c366-33faa8dd055f"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.6/85.6 kB\u001b[0m \u001b[31m424.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[91m━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/122.4 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m"]}],"source":["# Install required packages\n","!pip install -q transformers\n","!pip install -q peft==0.5.0\n","!pip install -q accelerate\n","!pip install -q bitsandbytes\n","!pip install -q sentencepiece\n","!pip install -q torch\n","!pip install -q pandas\n","!pip install -q numpy\n","!pip install -q scikit-learn\n","!pip install -q matplotlib\n","!pip install -q yfinance\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q-7J4RF_UVO1"},"outputs":[],"source":["import os\n","import torch\n","import pandas as pd\n","import yfinance as yf\n","from datetime import datetime, timedelta\n","from tqdm import tqdm\n","import warnings\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","from pydrive2.auth import GoogleAuth\n","from pydrive2.drive import GoogleDrive\n","warnings.filterwarnings('ignore')\n","\n","\n","class StockDataManager:\n","    def __init__(self):\n","        self.dirs = ['data', 'models', 'results']\n","        self.setup_directories()\n","\n","    def setup_directories(self):\n","        for dir_name in self.dirs:\n","            os.makedirs(dir_name, exist_ok=True)\n","            print(f\"Created/verified directory: {dir_name}/\")\n","\n","    # Download StockEmo data\n","    def download_stockemo_data(self):\n","        # Authenticate\n","        auth.authenticate_user()\n","        gauth = GoogleAuth()\n","        gauth.credentials = GoogleCredentials.get_application_default()\n","        drive = GoogleDrive(gauth)\n","\n","        file_ids = {\n","            \"train_stockemo.csv\": \"14kpQhdpjt57ySe9omZSofbmFF4iYUIDc\",\n","            \"val_stockemo.csv\": \"1-8FC0f1RDCNSkRt8doTDMAPrmdmazQ5u\",\n","            \"test_stockemo.csv\": \"1-A1n7mRMbje-me1rQpce_QsfFnlH0av7\",\n","            \"processed_stockemo.csv\": \"1-7QLxjVIezZLJ_Og5m3DmmW32BabnH2i\"\n","        }\n","\n","        for filename, file_id in file_ids.items():\n","            output_path = f\"data/{filename}\"\n","            if not os.path.exists(output_path):\n","                print(f\"Downloading {filename}...\")\n","                downloaded = drive.CreateFile({\"id\": file_id})\n","                downloaded.GetContentFile(output_path)\n","                print(f\"Saved to {output_path}\")\n","            else:\n","                print(f\"{filename} already exists\")\n","\n","    def get_unique_tickers_from_stockemo(self):\n","        \"\"\"Get list of unique tickers from processed_stockemo.csv\"\"\"\n","        try:\n","            processed_df = pd.read_csv(\"data/processed_stockemo.csv\")\n","            unique_tickers = processed_df['ticker'].unique().tolist()\n","            print(f\"Found {len(unique_tickers)} unique tickers in StockEmo data\")\n","            return unique_tickers\n","        except Exception as e:\n","            print(f\"Error reading StockEmo data: {str(e)}\")\n","            return []\n","\n","    def download_stock_data(self, start_date=None, end_date=None, period=None):\n","        \"\"\"\n","        Download stock data for all StockEmo tickers\n","        \"\"\"\n","        # Get symbols from StockEmo\n","        symbols = self.get_unique_tickers_from_stockemo()\n","\n","        if not symbols:\n","            raise ValueError(\"No tickers found in StockEmo data\")\n","\n","        # Handle dates\n","        if period and not (start_date or end_date):\n","            valid_periods = ['1d','5d','1mo','3mo','6mo','1y','2y','5y','10y','ytd','max']\n","            if period not in valid_periods:\n","                raise ValueError(f\"Invalid period. Must be one of {valid_periods}\")\n","        else:\n","            if not start_date:\n","                start_date = (datetime.now() - timedelta(days=365)).strftime('%Y-%m-%d')\n","            if not end_date:\n","                end_date = datetime.now().strftime('%Y-%m-%d')\n","\n","        stock_data = {}\n","        print(\"\\nDownloading stock data...\")\n","        for symbol in tqdm(symbols, desc=\"Downloading stock data\"):\n","            try:\n","                # Create Ticker object\n","                ticker = yf.Ticker(symbol)\n","\n","                # Get historical data\n","                if period:\n","                    hist = ticker.history(period=period)\n","                else:\n","                    hist = ticker.history(start=start_date, end=end_date)\n","\n","                if not hist.empty:\n","                    # Convert to DataFrame and reset index\n","                    df = hist.reset_index()\n","\n","                    # Add ticker column\n","                    df.insert(0, 'Ticker', symbol)\n","\n","                    # Save to CSV\n","                    output_path = f\"data/{symbol}_prices.csv\"\n","                    df.to_csv(output_path, index=False)\n","\n","                    stock_data[symbol] = df\n","                else:\n","                    print(f\"\\nNo data received for {symbol}\")\n","\n","            except Exception as e:\n","                print(f\"\\nError downloading {symbol}: {str(e)}\")\n","                continue\n","\n","        print(f\"\\nSuccessfully downloaded data for {len(stock_data)} tickers\")\n","        return stock_data\n","\n","def initialize_workspace(start_date=None, end_date=None, period=None):\n","    \"\"\"Initialize workspace and download stock data\"\"\"\n","    print(\"Setting up workspace...\")\n","    manager = StockDataManager()\n","\n","    manager.download_stockemo_data()\n","\n","    stock_data = manager.download_stock_data(\n","        start_date=start_date,\n","        end_date=end_date,\n","        period=period\n","    )\n","\n","    # Print summary of downloaded data\n","    print(\"\\nDownloaded Data Summary:\")\n","    for symbol, df in stock_data.items():\n","        print(f\"\\n{symbol}:\")\n","        print(f\"Date Range: {df['Date'].min()} to {df['Date'].max()}\")\n","        print(f\"Number of records: {len(df)}\")\n","\n","    return manager, stock_data\n","\n","if __name__ == \"__main__\":\n","    # Download data for all StockEmo tickers\n","    manager, data = initialize_workspace(\n","    start_date='2020-01-01',\n","    end_date='2020-12-31'\n",")\n","\n","    # Print total number of tickers downloaded\n","    print(f\"\\nTotal number of tickers downloaded: {len(data)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EdFd5ezlXxRN"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","from torch.utils.data import Dataset, DataLoader\n","import matplotlib.pyplot as plt\n","from typing import Dict, List, Tuple\n","import gc\n","\n","class SentimentDataset(Dataset):\n","    def __init__(self, processed_df, price_df, sequence_length, sentiment_model):\n","        processed_df['date'] = pd.to_datetime(processed_df['date'])\n","        price_df['Date'] = pd.to_datetime(price_df['Date'])\n","\n","        self.processed_df = processed_df.sort_values('date')\n","        self.price_df = price_df.sort_values('Date')\n","        self.sequence_length = sequence_length\n","        self.sentiment_model = sentiment_model\n","\n","        self.scaler = MinMaxScaler()\n","        self.prices_scaled = self.scaler.fit_transform(self.price_df['Close'].values.reshape(-1, 1)).flatten()\n","\n","    def __len__(self):\n","        return len(self.processed_df) - self.sequence_length\n","\n","    def __getitem__(self, idx):\n","        text = self.processed_df.iloc[idx]['processed']\n","        sentiment_score = self.sentiment_model.get_sentiment(text)\n","        features = torch.tensor([sentiment_score], dtype=torch.float32)\n","\n","        price_seq = torch.tensor(self.prices_scaled[idx:idx+self.sequence_length], dtype=torch.float32)\n","        target = torch.tensor(self.prices_scaled[idx+self.sequence_length], dtype=torch.float32)\n","\n","        return features, price_seq, target\n","\n","class SentimentModel:\n","    def __init__(self, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n","        self.device = device\n","        print(f\"Initializing FinBERT model on {device}\")\n","\n","        self.tokenizer = AutoTokenizer.from_pretrained('ProsusAI/finbert')\n","        self.model = AutoModelForSequenceClassification.from_pretrained(\n","            'ProsusAI/finbert'\n","        ).to(device)\n","        self.model.eval()\n","        print(\"Model loaded successfully!\")\n","\n","    def get_sentiment(self, text):\n","        try:\n","            inputs = self.tokenizer(\n","                text,\n","                return_tensors=\"pt\",\n","                truncation=True,\n","                max_length=512\n","            ).to(self.device)\n","\n","            with torch.no_grad():\n","                outputs = self.model(**inputs)\n","                probs = torch.softmax(outputs.logits, dim=1)\n","                score = probs[0][2].item() - probs[0][0].item()\n","            return score\n","        except Exception as e:\n","            print(f\"Error in sentiment analysis: {str(e)}\")\n","            return 0.0\n","\n","from transformers import LlamaTokenizerFast, LlamaForCausalLM\n","from peft import PeftModel\n","\n","class LlamaSentimentModel:\n","    def __init__(self, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n","        self.device = device\n","        print(f\"Initializing Llama-2 sentiment model on {device}\")\n","\n","        # Load tokenizer and model\n","        base_model = \"NousResearch/Llama-2-13b-hf\"\n","        peft_model = \"FinGPT/fingpt-sentiment_llama2-13b_lora\"\n","        self.tokenizer = LlamaTokenizerFast.from_pretrained(base_model, trust_remote_code=True)\n","        self.tokenizer.pad_token = self.tokenizer.eos_token\n","        self.model = LlamaForCausalLM.from_pretrained(\n","            base_model, trust_remote_code=True, device_map=device, load_in_8bit=True\n","        )\n","        self.model = PeftModel.from_pretrained(self.model, peft_model)\n","        self.model.eval()\n","        print(\"Llama-2 sentiment model loaded successfully!\")\n","\n","    def get_sentiment(self, text):\n","        prompt = f\"\"\"Instruction: What is the sentiment of this news? Please choose an answer from {{negative/neutral/positive}}\\nInput: {text}\\nAnswer: \"\"\"\n","        try:\n","            tokens = self.tokenizer(prompt, return_tensors=\"pt\", padding=True, max_length=512).to(self.device)\n","            with torch.no_grad():\n","                outputs = self.model.generate(**tokens, max_length=512)\n","            response = self.tokenizer.decode(outputs[0])\n","            sentiment = response.split(\"Answer: \")[-1].strip().lower()\n","            return sentiment\n","        except Exception as e:\n","            print(f\"Error in Llama sentiment analysis: {str(e)}\")\n","            return \"neutral\"  # Default sentiment in case of errors\n","\n","# Update SentimentDataset to incorporate both FinBERT and Llama sentiment scores\n","class UpdatedSentimentDataset(Dataset):\n","    def __init__(self, processed_df, price_df, sequence_length, finbert_model, llama_model):\n","        processed_df['date'] = pd.to_datetime(processed_df['date'])\n","        price_df['Date'] = pd.to_datetime(price_df['Date'])\n","\n","        self.processed_df = processed_df.sort_values('date')\n","        self.price_df = price_df.sort_values('Date')\n","        self.sequence_length = sequence_length\n","        self.finbert_model = finbert_model\n","        self.llama_model = llama_model\n","\n","        self.scaler = MinMaxScaler()\n","        self.prices_scaled = self.scaler.fit_transform(self.price_df['Close'].values.reshape(-1, 1)).flatten()\n","\n","    def __len__(self):\n","        return len(self.processed_df) - self.sequence_length\n","\n","    def __getitem__(self, idx):\n","        text = self.processed_df.iloc[idx]['processed']\n","        finbert_score = self.finbert_model.get_sentiment(text)\n","        llama_sentiment = self.llama_model.get_sentiment(text)\n","        llama_score = {\n","            \"positive\": 1.0,\n","            \"neutral\": 0.0,\n","            \"negative\": -1.0\n","        }.get(llama_sentiment, 0.0)  # Map Llama sentiment to numerical scores\n","\n","        features = torch.tensor([finbert_score, llama_score], dtype=torch.float32)\n","\n","        price_seq = torch.tensor(self.prices_scaled[idx:idx+self.sequence_length], dtype=torch.float32)\n","        target = torch.tensor(self.prices_scaled[idx+self.sequence_length], dtype=torch.float32)\n","\n","        return features, price_seq, target\n","\n","def analyze_sentiments_by_ticker(processed_df, sentiment_model, llama_model, batch_size=32):\n","    results = []\n","    processed_df['date'] = pd.to_datetime(processed_df['date'])\n","    grouped = processed_df.groupby('ticker')\n","\n","    print(\"\\nAnalyzing sentiments for each ticker...\")\n","    for ticker, group in tqdm(grouped, desc=\"Processing tickers\"):\n","        ticker_results = []\n","\n","        for i in range(0, len(group), batch_size):\n","            batch = group.iloc[i:i + batch_size]\n","\n","            for _, row in batch.iterrows():\n","                text = row['processed']\n","                sentiment_score = sentiment_model.get_sentiment(text)\n","\n","                ticker_results.append({\n","                    'date': row['date'],\n","                    'ticker': ticker,\n","                    'text': text,\n","                    'sentiment': sentiment_score,\n","                    'original_sentiment': row['senti_label'],\n","                    'industry': row['industry']\n","                })\n","\n","        ticker_df = pd.DataFrame(ticker_results)\n","        ticker_df.to_csv(f\"data/{ticker}_sentiment_analysis.csv\", index=False)\n","        results.extend(ticker_results)\n","\n","    return pd.DataFrame(results)\n","\n","def main():\n","    print(\"Initializing stock data download...\")\n","    manager = StockDataManager()\n","    stock_data = manager.download_stock_data(start_date='2020-01-01',end_date='2020-12-31')\n","    print(\"\\nLoading StockEmo data...\")\n","    processed_df = pd.read_csv(\"data/processed_stockemo.csv\")\n","\n","    sentiment_model = SentimentModel()\n","    llama_model = LlamaSentimentModel()\n","    results_df = analyze_sentiments_by_ticker(processed_df, sentiment_model, llama_model)\n","    results_df.to_csv(\"data/all_tickers_sentiment_analysis.csv\", index=False)\n","    print(\"\\nSaved sentiment analysis results to data/all_tickers_sentiment_analysis.csv\")\n","\n","    return sentiment_model, llama_model, results_df, stock_data\n","\n","if __name__ == \"__main__\":\n","    sentiment_model, llama_model, sentiment_results, stock_data = main()\n","\n","    print(\"\\nAnalysis Summary:\")\n","    print(f\"Number of tickers analyzed: {len(sentiment_results['ticker'].unique())}\")\n","    print(\"\\nSentiment distribution:\")\n","    print(sentiment_results['sentiment'].describe())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kzIhyl7Jpe9y"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","import matplotlib.pyplot as plt\n","from typing import Dict, List, Tuple\n","import os\n","\n","class SentimentLSTM(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers, dropout=0.2):\n","        super().__init__()\n","\n","        self.hidden_dim = hidden_dim\n","        self.num_layers = num_layers\n","\n","        self.lstm = nn.LSTM(\n","            input_dim,\n","            hidden_dim,\n","            num_layers,\n","            batch_first=True,\n","            dropout=dropout if num_layers > 1 else 0\n","        )\n","\n","        self.fc = nn.Linear(hidden_dim, 1)\n","\n","    def forward(self, x_sentiment, x_price):\n","        batch_size = x_price.size(0)\n","        seq_length = x_price.size(1)\n","\n","        x_sentiment = x_sentiment.unsqueeze(1).repeat(1, seq_length, 1)\n","        x_combined = torch.cat((x_sentiment, x_price.unsqueeze(-1)), dim=2)\n","\n","        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x_combined.device)\n","        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x_combined.device)\n","\n","        out, _ = self.lstm(x_combined, (h0, c0))\n","        out = self.fc(out[:, -1, :])\n","        return out.squeeze()\n","\n","class BaselineLSTM(nn.Module):\n","    def __init__(self, hidden_dim, num_layers, dropout=0.2):\n","        super().__init__()\n","\n","        self.hidden_dim = hidden_dim\n","        self.num_layers = num_layers\n","\n","        self.lstm = nn.LSTM(\n","            1,  # Only price input\n","            hidden_dim,\n","            num_layers,\n","            batch_first=True,\n","            dropout=dropout if num_layers > 1 else 0\n","        )\n","\n","        self.fc = nn.Linear(hidden_dim, 1)\n","\n","    def forward(self, x_price):\n","        batch_size = x_price.size(0)\n","\n","        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x_price.device)\n","        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x_price.device)\n","\n","        out, _ = self.lstm(x_price.unsqueeze(-1), (h0, c0))\n","        out = self.fc(out[:, -1, :])\n","        return out.squeeze()\n","\n","class TickerDataset(Dataset):\n","    def __init__(self, price_df, sentiment_df, sequence_length):\n","        price_df = price_df.copy()\n","        sentiment_df = sentiment_df.copy()\n","\n","        # Convert price dates to YYYY-MM-DD\n","        price_df['Date'] = price_df['Date'].astype(str).str[:10]\n","\n","        # # Debug print\n","        # print(\"\\nPrice dates sample:\", price_df['Date'].head().tolist())\n","        # print(\"Sentiment dates sample:\", sentiment_df['date'].head().tolist())\n","        # print(\"\\nTotal unique price dates:\", len(price_df['Date'].unique()))\n","        # print(\"Total unique sentiment dates:\", len(sentiment_df['date'].unique()))\n","        # print(\"\\nSample overlap:\", len(set(price_df['Date']) & set(sentiment_df['date'])))\n","\n","        self.price_df = price_df[price_df['Date'].isin(sentiment_df['date'])]\n","        self.sentiment_df = sentiment_df\n","\n","        print(f\"Matched data points: {len(self.price_df)}\")\n","\n","        if len(self.price_df) < sequence_length + 1:\n","            raise ValueError(f\"Insufficient data points. Need at least {sequence_length + 1} points, got {len(self.price_df)}\")\n","\n","        self.scaler = MinMaxScaler()\n","        self.prices_scaled = self.scaler.fit_transform(self.price_df['Close'].values.reshape(-1, 1)).flatten()\n","\n","        self.sequence_length = sequence_length\n","        self.sentiments = self.sentiment_df['sentiment'].values\n","\n","    def __len__(self):\n","        return len(self.prices_scaled) - self.sequence_length - 1\n","\n","    def __getitem__(self, idx):\n","        sentiment = torch.tensor([self.sentiments[idx]], dtype=torch.float32)\n","        price_seq = torch.tensor(self.prices_scaled[idx:idx+self.sequence_length], dtype=torch.float32)\n","        target = torch.tensor(self.prices_scaled[idx+self.sequence_length], dtype=torch.float32)\n","        return sentiment, price_seq, target\n","\n","    def inverse_transform(self, scaled_values):\n","        return self.scaler.inverse_transform(scaled_values.reshape(-1, 1)).flatten()\n","\n","class BaselineDataset(Dataset):\n","    def __init__(self, price_df, sequence_length):\n","        self.price_df = price_df.copy()\n","        self.sequence_length = sequence_length\n","\n","        self.scaler = MinMaxScaler()\n","        self.prices_scaled = self.scaler.fit_transform(self.price_df['Close'].values.reshape(-1, 1)).flatten()\n","\n","    def __len__(self):\n","        return len(self.prices_scaled) - self.sequence_length - 1\n","\n","    def __getitem__(self, idx):\n","        price_seq = torch.tensor(self.prices_scaled[idx:idx+self.sequence_length], dtype=torch.float32)\n","        target = torch.tensor(self.prices_scaled[idx+self.sequence_length], dtype=torch.float32)\n","        return price_seq, target\n","\n","    def inverse_transform(self, scaled_values):\n","        return self.scaler.inverse_transform(scaled_values.reshape(-1, 1)).flatten()\n","\n","\n","def validate_data_files(tickers: List[str]) -> List[str]:\n","    valid_tickers = []\n","    for ticker in tickers:\n","        sentiment_path = f\"data/{ticker}_sentiment_analysis.csv\"\n","        price_path = f\"data/{ticker}_prices.csv\"\n","\n","        if os.path.exists(sentiment_path) and os.path.exists(price_path):\n","            try:\n","                sentiment_df = pd.read_csv(sentiment_path)\n","                price_df = pd.read_csv(price_path)\n","                if len(sentiment_df) > 0 and len(price_df) > 0:\n","                    valid_tickers.append(ticker)\n","                else:\n","                    print(f\"Warning: Empty data files for {ticker}\")\n","            except Exception as e:\n","                print(f\"Error reading data for {ticker}: {str(e)}\")\n","        else:\n","            print(f\"Warning: Missing data files for {ticker}\")\n","\n","    print(f\"Found {len(valid_tickers)} valid tickers out of {len(tickers)} total\")\n","    return valid_tickers\n","\n","def train_and_evaluate_ticker(ticker: str, model_params: Dict, train_params: Dict) -> Tuple[List[float], List[float]]:\n","    price_df = pd.read_csv(f\"data/{ticker}_prices.csv\")\n","    sentiment_df = pd.read_csv(f\"data/{ticker}_sentiment_analysis.csv\")\n","\n","    dataset = TickerDataset(price_df, sentiment_df, model_params['sequence_length'])\n","\n","    train_size = int(0.8 * len(dataset))\n","    val_size = len(dataset) - train_size\n","    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n","\n","    train_loader = DataLoader(train_dataset, batch_size=train_params['batch_size'], shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=train_params['batch_size'])\n","\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model = SentimentLSTM(\n","        model_params['input_dim'],\n","        model_params['hidden_dim'],\n","        model_params['num_layers']\n","    ).to(device)\n","\n","    criterion = nn.MSELoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=train_params['learning_rate'])\n","\n","    for epoch in range(train_params['epochs']):\n","        model.train()\n","        train_loss = 0\n","        for sentiment, price_seq, target in train_loader:\n","            sentiment, price_seq, target = sentiment.to(device), price_seq.to(device), target.to(device)\n","\n","            optimizer.zero_grad()\n","            output = model(sentiment, price_seq)\n","            loss = criterion(output, target)\n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item()\n","\n","        if epoch % 10 == 0:\n","            print(f'{ticker} - Epoch {epoch}: Loss = {train_loss/len(train_loader):.4f}')\n","\n","    model.eval()\n","    predictions = []\n","    actuals = []\n","\n","    with torch.no_grad():\n","        for sentiment, price_seq, target in val_loader:\n","            sentiment, price_seq = sentiment.to(device), price_seq.to(device)\n","            output = model(sentiment, price_seq)\n","            predictions.extend(output.cpu().numpy())\n","            actuals.extend(target.numpy())\n","\n","    predictions = dataset.inverse_transform(np.array(predictions))\n","    actuals = dataset.inverse_transform(np.array(actuals))\n","\n","    return predictions, actuals\n","\n","def train_and_evaluate_baseline(ticker: str, model_params: Dict, train_params: Dict) -> Tuple[List[float], List[float]]:\n","    price_df = pd.read_csv(f\"data/{ticker}_prices.csv\")\n","    price_df['Date'] = price_df['Date'].astype(str).str[:10]\n","\n","    dataset = BaselineDataset(price_df, model_params['sequence_length'])\n","\n","    train_size = int(0.8 * len(dataset))\n","    val_size = len(dataset) - train_size\n","    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n","\n","    train_loader = DataLoader(train_dataset, batch_size=train_params['batch_size'], shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=train_params['batch_size'])\n","\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model = BaselineLSTM(\n","        model_params['hidden_dim'],\n","        model_params['num_layers']\n","    ).to(device)\n","\n","    criterion = nn.MSELoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=train_params['learning_rate'])\n","\n","    for epoch in range(train_params['epochs']):\n","        model.train()\n","        train_loss = 0\n","        for price_seq, target in train_loader:\n","            price_seq, target = price_seq.to(device), target.to(device)\n","\n","            optimizer.zero_grad()\n","            output = model(price_seq)\n","            loss = criterion(output, target)\n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item()\n","\n","        if epoch % 10 == 0:\n","            print(f'{ticker} Baseline - Epoch {epoch}: Loss = {train_loss/len(train_loader):.4f}')\n","\n","    model.eval()\n","    predictions = []\n","    actuals = []\n","\n","    with torch.no_grad():\n","        for price_seq, target in val_loader:\n","            price_seq = price_seq.to(device)\n","            output = model(price_seq)\n","            predictions.extend(output.cpu().numpy())\n","            actuals.extend(target.numpy())\n","\n","    predictions = dataset.inverse_transform(np.array(predictions))\n","    actuals = dataset.inverse_transform(np.array(actuals))\n","\n","    return predictions, actuals\n","\n","def plot_comparison(ticker: str, sentiment_pred: List[float], baseline_pred: List[float], actuals: List[float]):\n","    plt.figure(figsize=(15, 10))\n","\n","    # Price Comparison Plot\n","    plt.plot(actuals, label='Actual', color='blue', linewidth=2)\n","    plt.plot(sentiment_pred, label='LSTM with Sentiment', color='red', linestyle='--', linewidth=2)\n","    plt.plot(baseline_pred, label='Baseline LSTM', color='green', linestyle='--', linewidth=2)\n","\n","    plt.title(f'Price Predictions Comparison for {ticker}')\n","    plt.xlabel('Time Steps')\n","    plt.ylabel('Price')\n","    plt.legend()\n","    plt.grid(True)\n","\n","    plt.tight_layout()\n","    plt.savefig(f'results/{ticker}_comparison.png')\n","    plt.close()\n","\n","\n","def plot_results(ticker: str, predictions: List[float], actuals: List[float]):\n","    plt.figure(figsize=(15, 10))\n","\n","    # Create subplots\n","    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12))\n","\n","    # Price Comparison Plot\n","    ax1.plot(actuals, label='Actual', color='blue', linewidth=2)\n","    ax1.plot(predictions, label='Predicted', color='red', linestyle='--', linewidth=2)\n","    ax1.set_title(f'Price Predictions vs Actual for {ticker}')\n","    ax1.set_xlabel('Time Steps')\n","    ax1.set_ylabel('Price')\n","    ax1.legend()\n","    ax1.grid(True)\n","\n","    # Error Analysis Plot\n","    errors = np.array(predictions) - np.array(actuals)\n","    ax2.plot(errors, color='green', label='Prediction Error')\n","    ax2.axhline(y=0, color='r', linestyle='-', alpha=0.3)\n","    ax2.set_title('Prediction Error Over Time')\n","    ax2.set_xlabel('Time Steps')\n","    ax2.set_ylabel('Error (Predicted - Actual)')\n","    ax2.legend()\n","    ax2.grid(True)\n","\n","    plt.tight_layout()\n","    plt.savefig(f'results/{ticker}_analysis.png')\n","    plt.close()\n","\n","def plot_mse_comparison(results_df):\n","    # Filter successful predictions only\n","    df = results_df[results_df['status'] == 'success']\n","\n","    plt.figure(figsize=(15, 8))\n","\n","    # Create grouped bars for MSE comparison\n","    tickers = df.index\n","    x = np.arange(len(tickers))\n","    width = 0.35\n","\n","    plt.bar(x - width/2, df['sentiment_mse'], width, label='LSTM with Sentiment', color='blue', alpha=0.7)\n","    plt.bar(x + width/2, df['baseline_mse'], width, label='Baseline LSTM', color='red', alpha=0.7)\n","\n","    plt.xlabel('Tickers')\n","    plt.ylabel('Mean Squared Error (MSE)')\n","    plt.title('MSE Comparison: Sentiment LSTM vs Baseline LSTM')\n","    plt.xticks(x, tickers, rotation=45)\n","    plt.legend()\n","    plt.grid(True, alpha=0.3)\n","    plt.tight_layout()\n","\n","    plt.savefig('results/mse_comparison.png')\n","    plt.close()\n","\n","    # Calculate and print average improvement\n","    avg_improvement = df['improvement'].mean()\n","    print(f\"\\nAverage MSE Improvement: {avg_improvement:.2f}%\")\n","\n","    # Print individual MSE comparisons\n","    print(\"\\nMSE Comparison by Ticker:\")\n","    for ticker in df.index:\n","        print(f\"{ticker}:\")\n","        print(f\"  Sentiment LSTM MSE: {df.loc[ticker, 'sentiment_mse']:.6f}\")\n","        print(f\"  Baseline LSTM MSE: {df.loc[ticker, 'baseline_mse']:.6f}\")\n","        print(f\"  Improvement: {df.loc[ticker, 'improvement']:.2f}%\")\n","\n","\n","def main():\n","    model_params = {\n","        'input_dim': 2, #2 for sentiment, 1 for baseline\n","        'hidden_dim': 25,  # Best performing hidden size\n","        'num_layers': 2,\n","        'sequence_length': 5  # Best performing window size\n","    }\n","\n","    train_params = {\n","        'batch_size': 32,\n","        'epochs': 250,  # Best performing epoch count\n","        'learning_rate': 0.001,\n","        'train_split': 0.67  # 67% train, 33% test\n","    }\n","\n","    all_tickers = [f.split('_')[0] for f in os.listdir('data') if f.endswith('_sentiment_analysis.csv')]\n","    valid_tickers = validate_data_files(all_tickers)\n","\n","    os.makedirs('results', exist_ok=True)\n","\n","    results = {}\n","    for ticker in valid_tickers:\n","        print(f\"\\nProcessing {ticker}\")\n","        try:\n","            sentiment_pred, sentiment_actuals = train_and_evaluate_ticker(ticker, model_params, train_params)\n","            baseline_pred, baseline_actuals = train_and_evaluate_baseline(ticker, model_params, train_params)\n","\n","            sentiment_mse = np.mean((sentiment_pred - sentiment_actuals) ** 2)\n","            sentiment_mae = np.mean(np.abs(sentiment_pred - sentiment_actuals))\n","            baseline_mse = np.mean((baseline_pred - baseline_actuals) ** 2)\n","            baseline_mae = np.mean(np.abs(baseline_pred - baseline_actuals))\n","\n","            results[ticker] = {\n","                'sentiment_mse': sentiment_mse,\n","                'sentiment_mae': sentiment_mae,\n","                'baseline_mse': baseline_mse,\n","                'baseline_mae': baseline_mae,\n","                'improvement': ((baseline_mse - sentiment_mse) / baseline_mse) * 100,\n","                'status': 'success'\n","            }\n","\n","            plot_results(ticker, sentiment_pred, sentiment_actuals)\n","            plot_comparison(ticker, sentiment_pred, baseline_pred, sentiment_actuals)\n","\n","        except Exception as e:\n","            print(f\"Error processing {ticker}: {str(e)}\")\n","            results[ticker] = {\n","                'sentiment_mse': None,\n","                'sentiment_mae': None,\n","                'baseline_mse': None,\n","                'baseline_mae': None,\n","                'improvement': None,\n","                'status': f'failed: {str(e)}'\n","            }\n","\n","    results_df = pd.DataFrame.from_dict(results, orient='index')\n","    results_df.to_csv('results/model_comparison_metrics.csv')\n","    plot_mse_comparison(results_df)\n","\n","\n","    success_count = sum(1 for v in results.values() if v['status'] == 'success')\n","    print(f\"\\nProcessing complete:\")\n","    print(f\"Successfully processed: {success_count}/{len(valid_tickers)} tickers\")\n","    print(f\"Results saved to: results/model_comparison_metrics.csv\")\n","\n","if __name__ == '__main__':\n","    main()"]},{"cell_type":"code","source":["from google.colab import files\n","\n","# Zip and download each folder separately\n","folders = ['data', 'models', 'results']\n","\n","for folder in folders:\n","    if os.path.exists(folder):  # Check if folder exists\n","        zip_name = f'{folder}.zip'\n","        !zip -r {zip_name} {folder}\n","        files.download(zip_name)\n","        !rm {zip_name}  # Clean up the zip file after download\n","    else:\n","        print(f\"Folder '{folder}' not found\")"],"metadata":{"id":"ORfSx7hHkWAc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","!pwd\n","\n","# Copy folders to the current Colab directory\n","notebook_dir = \"/content\"  # Replace with your actual path\n","!cp -r data \"{notebook_dir}/FinGPT/\"\n","!cp -r models \"{notebook_dir}/FinGPT/\"\n","!cp -r results \"{notebook_dir}/FinGPT/\""],"metadata":{"id":"eruFq8cpkXCv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!zip -r /content/FinGPT.zip /content/FinGPT"],"metadata":{"id":"XoVoWen8lPFG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import files\n","files.download(\"/content/FinGPT.zip\")"],"metadata":{"id":"h5fka-PJlc7w"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1TiXlgMeslWjf3K1aUlCxTnRUrpva-LxV","timestamp":1732305039758}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}